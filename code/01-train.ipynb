{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple classifier\n",
    "\n",
    "> This notebook produces the same result as `train-model.py`. The only difference is that this is a notebook environment (and requires additional dependencies to run). You can run either file interchangeably to produce the Machine Learning model we'll place in the AWS Lambda.\n",
    "\n",
    "This notebook will fit a gradient boosted ensemble of trees to the infamous 1995 _breast cancer_ dataset. The goal is to produce a model that can somewhat-accurately predict if a patient has breast cancer or not.\n",
    "\n",
    "We will then export the fitted model and deploy it using AWS Lambda and API Gateway to enable it for online consumption."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note about your environment\n",
    "\n",
    "Run this notebook with the package versions specified in `requirements.txt`!\n",
    "\n",
    "This is extremely important because scikit-learn is so heavy that it will not fit in the Lambda's deployment package. Hence, we will use a very specific Lambda Layer that is only compatible with these specifications. If you are use another configuration (i.e., scikit-learn version 1.0.0), your Lambda may not be able to load the final model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The dataset has 560 observations, 30 features and one target.\n",
    "\n",
    "By default, the target is encoded as:\n",
    "- `0` for malignant tumors; and\n",
    "- `1` for benign tumors.\n",
    "\n",
    "We will flip the labels so that `1` implies malignant (line 12 in the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "all_data = load_breast_cancer()\n",
    "\n",
    "# Features to pandas\n",
    "X = pd.DataFrame(\n",
    "    data=all_data['data'],\n",
    "    columns=all_data['feature_names']\n",
    ")\n",
    "\n",
    "# Target to pandas\n",
    "y = pd.DataFrame(\n",
    "    data=(1 - all_data['target']), # Flip target labels\n",
    "    columns=['malignant']\n",
    ")\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We will now instantiate a classifier with some generic hyperparameters. We will then recursively fit this model on the data, and in each step, we will drop the least important feature. We do this to limit the number of features needed to make a good prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a splitter (used in cross validation)\n",
    "cv_splitter = KFold(\n",
    "    n_splits=10,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "clf = GradientBoostingClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=1000,\n",
    "    max_depth=3,\n",
    "    max_features=9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Instantiate feature eliminator\n",
    "rfe = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    min_features_to_select=5,\n",
    "    cv=cv_splitter,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit many models, each with less features\n",
    "rfe = rfe.fit(\n",
    "    X=X_train,\n",
    "    y=y_train['malignant']\n",
    ")\n",
    "\n",
    "# Store top-five features\n",
    "cols = rfe.get_feature_names_out()\n",
    "\n",
    "# Print results\n",
    "print(f'The optimal model uses {rfe.n_features_} features:\\n{rfe.get_feature_names_out()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Now that we have found the best subset of features for the basic model, we will play around with its hyperparameters to find the best overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter candidates\n",
    "grid = {\n",
    "    'n_estimators': [500, 1000, 1500, 2000],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Instantiate search\n",
    "search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=grid,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    cv=cv_splitter\n",
    ")\n",
    "\n",
    "# Find best combination\n",
    "search = search.fit(\n",
    "    X_train[cols],\n",
    "    y_train['malignant']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the CV results to determine the best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in pandas df\n",
    "search_res = pd.DataFrame(\n",
    "    data=search.cv_results_)[\n",
    "        [\n",
    "            'params',\n",
    "            'mean_test_score',\n",
    "            'std_test_score',\n",
    "            'rank_test_score'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "# View top-five models\n",
    "search_res.sort_values('rank_test_score').head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model\n",
    "Now that we know both the best subset of features and hyperparameters, we can persist the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Set parameters\n",
    "clf = clf.set_params(\n",
    "    **search.best_estimator_.get_params()\n",
    ")\n",
    "\n",
    "# Fit on whole training dataset\n",
    "clf.fit(\n",
    "    X_train[cols],\n",
    "    y_train['malignant']\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "pred_train = clf.predict(X_train[cols])\n",
    "pred_test = clf.predict(X_test[cols])\n",
    "\n",
    "# Score predictions\n",
    "f1_train = f1_score(\n",
    "    y_true=y_train,\n",
    "    y_pred=pred_train\n",
    ")\n",
    "f1_test = f1_score(\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_test\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f'F1-score on training data: {round(f1_train, 2)}')\n",
    "print(f'F1-score on testing data: {round(f1_test, 2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell that our model is slightly overfitting on the training data because the F1-score decreased by four percentage points on the testing dataset.\n",
    "\n",
    "Regardless, this is a somewhat decent result, so we'll proceed to train the model on all the available data and export it using `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on all data\n",
    "clf = clf.fit(\n",
    "    X=X_train[cols].values, # Train without feature names\n",
    "    y=y_train['malignant']\n",
    ")\n",
    "\n",
    "# Export model\n",
    "pickle.dump(\n",
    "    obj=clf,\n",
    "    file=open('../results/clf.sav', 'wb')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction, the final model (`clf`) expects five inputs:\n",
    "1. Mean concave points;\n",
    "2. Worst radius;\n",
    "3. Worst texture;\n",
    "4. Worst area; and\n",
    "5. Worst concave points.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(\n",
    "    X=[\n",
    "        [\n",
    "            0.19,\n",
    "            33.13,\n",
    "            23.58,\n",
    "            3234.0,\n",
    "            0.28\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
